{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNw8iRVo2b+Ai5/EDndzMpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnelson05/Sentiment-Analysis-Model/blob/main/SentimentAnalysisModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Choose a Dataset**\n",
        "You can pick: (or try some other ones you find interesting):\n",
        "IMDB Movie Reviews (sentiment labels: positive/negative).\n",
        "Yelp Reviews (sentiment labels: star ratings or binary positive/negative).\n",
        "For now, complete the rest of the steps (2-4) below with the above two datasets. \\\\\n",
        "\n",
        "Come back to do the following task after you're done with the above (time permitting):\n",
        "\n",
        "We are going to use the Amazon Product Reviews (various categories, can be collapsed into positive/negative) dataset. You are free to decide how to collapse multiple categories into one. You can also compare different approaches of this as well.\n",
        "Feel free to use the datasets library (e.g., load_dataset(\"imdb\"))."
      ],
      "metadata": {
        "id": "_DTvLc0Gxd0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# IMDB Movie Review Dataset\n",
        "imdb_ds = load_dataset(\"imdb\")\n",
        "# Yelp Reviews Dataset\n",
        "yelp_ds = load_dataset(\"yelp_polarity\")\n",
        "\n",
        "print(\"IMBD Dataset\")\n",
        "print(imdb_ds)\n",
        "\n",
        "print(\"Yelp Dataset\")\n",
        "print(yelp_ds)"
      ],
      "metadata": {
        "id": "ipuRxmY9xfqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbc4306-b90b-443c-af11-65b70b14b71b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMBD Dataset\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "Yelp Dataset\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 560000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 38000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Select Two or More Pretrained Models**\n",
        "Pick at least two from the Hugging Face Hub and compare them:\n",
        "DistilBERT (e.g., distilbert-base-uncased-finetuned-sst-2-english)\n",
        "BERT (e.g., bert-base-uncased-finetuned-sst-2-english)\n",
        "RoBERTa (e.g., cardiffnlp/twitter-roberta-base-sentiment-latest or roberta-base-openai-detector)\n",
        "Feel free to explore the Hugging Face Model Hub if you find something else interesting!"
      ],
      "metadata": {
        "id": "VnHzYRapxiQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# RoBERTa\n",
        "roberta_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "# DistilBERT\n",
        "distilbert_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "sample_review_1 = \"This movie was great!\"\n",
        "sample_review_2 = \"I didn't like the actors.\"\n",
        "sample_review_3 = \"This movie was okay.\"\n",
        "\n",
        "roberta_result_1 = roberta_pipeline(sample_review_1)\n",
        "roberta_result_2 = roberta_pipeline(sample_review_2)\n",
        "roberta_result_3 = roberta_pipeline(sample_review_3)\n",
        "\n",
        "distilbert_result_1 = distilbert_pipeline(sample_review_1)\n",
        "distilbert_result_2 = distilbert_pipeline(sample_review_2)\n",
        "distilbert_result_3 = distilbert_pipeline(sample_review_3)\n",
        "\n",
        "print(\"RoBERTa Predictions:\")\n",
        "print(f\"Review 1: {roberta_result_1}\")\n",
        "print(f\"Review 2: {roberta_result_2}\")\n",
        "print(f\"Review 3: {roberta_result_3}\")\n",
        "\n",
        "print(\"\\nDistilBERT Predictions:\")\n",
        "print(f\"Review 1: {distilbert_result_1}\")\n",
        "print(f\"Review 2: {distilbert_result_2}\")\n",
        "print(f\"Review 3: {distilbert_result_3}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "F2B1q_Fdximf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e880fd-4542-46fa-927f-5e8c7a9f0169"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Predictions:\n",
            "Review 1: [{'label': 'positive', 'score': 0.9858884215354919}]\n",
            "Review 2: [{'label': 'negative', 'score': 0.8923878073692322}]\n",
            "Review 3: [{'label': 'positive', 'score': 0.9490081071853638}]\n",
            "\n",
            "DistilBERT Predictions:\n",
            "Review 1: [{'label': 'POSITIVE', 'score': 0.9998677968978882}]\n",
            "Review 2: [{'label': 'NEGATIVE', 'score': 0.9989005327224731}]\n",
            "Review 3: [{'label': 'POSITIVE', 'score': 0.9997859597206116}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Measure Performance**\n",
        "Implement an evaluation method on a test or validation split. At minimum:\\\n",
        "Accuracy: The fraction of examples predicted correctly.\\\n",
        "F1 Score: Combination of precision and recall. (explanation of this is given below, after the instructions)\\\n",
        "You can use the Hugging Face evaluate or datasets library or write your own small function for computing these metrics."
      ],
      "metadata": {
        "id": "MGdLnjmJxlRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def label_to_num(sentiment):\n",
        "  if (sentiment == \"POSITIVE\" or sentiment == \"positive\"):\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "test_split_imdb = imdb_ds[\"test\"].select(range(1000))\n",
        "imdb_text = [text[:500] for text in test_split_imdb[\"text\"]]\n",
        "\n",
        "# RoBERTa predictions for the test sample, converted to 1s and 0s\n",
        "roberta_predictions = roberta_pipeline(imdb_text)\n",
        "roberta_prediction_nums = [label_to_num(pred[\"label\"]) for pred in roberta_predictions]\n",
        "\n",
        "# DistilBERT predictions for the test sample, converted to 1s and 0s\n",
        "distilbert_predictions = distilbert_pipeline(imdb_text)\n",
        "distilbert_prediction_nums = [label_to_num(pred[\"label\"]) for pred in distilbert_predictions]\n",
        "\n",
        "# Hugging Face Accuracy Documentation: https://huggingface.co/spaces/evaluate-metric/accuracy\n",
        "# Hugging Face F1 Score Documentation: https://huggingface.co/spaces/evaluate-metric/f1\n",
        "\n",
        "roberta_accuracy = accuracy.compute(predictions = roberta_prediction_nums, references = test_split_imdb[\"label\"])\n",
        "roberta_f1_score = f1.compute(predictions = roberta_prediction_nums, references = test_split_imdb[\"label\"])\n",
        "\n",
        "distilbert_accuracy = accuracy.compute(predictions = distilbert_prediction_nums, references = test_split_imdb[\"label\"])\n",
        "distilbert_f1_score = f1.compute(predictions = distilbert_prediction_nums, references = test_split_imdb[\"label\"])\n",
        "\n",
        "# Getting F1 score of zero\n",
        "\n",
        "print(\"RoBERTa Model Evaluation for IMDB Set:\")\n",
        "print(f\"Accuracy: {roberta_accuracy['accuracy']}\")\n",
        "print(f\"F1 Score: {roberta_f1_score['f1']}\")\n",
        "print(\"DistilBERT Model Evaluation for IMDB Set:\")\n",
        "print(f\"Accuracy: {distilbert_accuracy['accuracy']}\")\n",
        "print(f\"F1 Score: {distilbert_f1_score['f1']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjMD8eQTrelw",
        "outputId": "0ee6f838-f1a1-4525-d123-39759ce624cf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Model Evaluation for IMDB Set:\n",
            "Accuracy: 0.868\n",
            "F1 Score: 0.0\n",
            "DistilBERT Model Evaluation for IMDB Set:\n",
            "Accuracy: 0.837\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Compare Models & Do a Short Error Analysis**\n",
        "After running inference on your test set:\n",
        "Compare Metrics: Which model is most accurate overall? Does one have higher F1?\n",
        "Identify Edge Cases:\n",
        "Look at ~5 examples that were misclassified by at least one model.\n",
        "What patterns do you see? (e.g., tricky wording, sarcasm, short text, etc.) If you don't see any patter that's fine but make sure you've looked hard enough (maybe you need more than 5 examples?)\n",
        "What examples do all models make mistakes on? What mistakes are unique to a particular model? (again, if you dont find a pattern that's fine but make sure you've tried a lot of things)\n",
        "Write a short paragraph or make a small table summarizing your findings:\n",
        "Which model performed best overall?\n",
        "Any surprising differences?\n",
        "How might you improve performance further?"
      ],
      "metadata": {
        "id": "08ijNkM1xoyj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bsH9CNRGxpEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deliverables**\n",
        "**Code**: A Python script or Jupyter notebook showing how you: \\\\\n",
        "Load data (and potentially preprocess it). \\\\\n",
        "Instantiate the Hugging Face pipelines. \\\\\n",
        "Run predictions and calculate metrics. \\\\\n",
        "\n",
        "**Short text on findings**  ( in a text box in your Colab notebook): \\\\\n",
        "Which dataset(s) you chose and why. \\\\\n",
        "The models you compared and a table/plot of accuracy or F1 (use matplotlib for graphs). \\\\\n",
        "Example misclassified cases and your hypothesis for why they failed. \\\\"
      ],
      "metadata": {
        "id": "GhIe5pyfxwmo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpF24xp6xw5p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}